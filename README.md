# LLM Training Data Pipeline

A production-ready data pipeline for preparing text data for Large Language Model training. Built with Python, designed for scalability and reproducibility.

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        LLM Training Data Pipeline                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚          â”‚    â”‚          â”‚    â”‚          â”‚    â”‚          â”‚    â”‚        â”‚â”‚
â”‚  â”‚ Ingest   â”‚â”€â”€â”€â–¶â”‚  Clean   â”‚â”€â”€â”€â–¶â”‚  Dedup   â”‚â”€â”€â”€â–¶â”‚ Quality  â”‚â”€â”€â”€â–¶â”‚Tokenizeâ”‚â”‚
â”‚  â”‚          â”‚    â”‚          â”‚    â”‚          â”‚    â”‚  Filter  â”‚    â”‚        â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”‚       â”‚              â”‚               â”‚               â”‚               â”‚      â”‚
â”‚       â–¼              â–¼               â–¼               â–¼               â–¼      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚                         Metrics & Logging                               â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“Š Pipeline Stages

| Stage | Description | Key Metrics |
|-------|-------------|-------------|
| **Ingestion** | Download Wikipedia dumps, parse XML | Documents ingested, bytes processed |
| **Cleaning** | Remove markup, normalize text | Characters removed, encoding fixes |
| **Deduplication** | MinHash LSH for near-duplicate detection | Duplicates found, unique documents |
| **Quality Filter** | Length, language, perplexity filtering | Documents filtered by reason |
| **Tokenization** | BPE tokenization, vocabulary stats | Token counts, vocab coverage |

## ğŸš€ Quick Start

### Prerequisites
- Python 3.9+
- Docker (optional)

### Installation

```bash
# Clone the repository
git clone https://github.com/yourusername/llm-data-pipeline.git
cd llm-data-pipeline

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

### Run the Pipeline

```bash
# Download sample data (Simple English Wikipedia)
python -m src.ingestion.download_wiki

# Run full pipeline
python -m src.main

# Or run individual stages
python -m src.processing.cleaner
python -m src.processing.deduplicator
python -m src.processing.quality_filter
python -m src.processing.tokenizer
```

### Docker

```bash
# Build and run
docker-compose up --build

# Run pipeline inside container
docker-compose exec pipeline python -m src.main
```

## ğŸ“ Project Structure

```
llm-data-pipeline/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ ingestion/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ download_wiki.py      # Wikipedia dump downloader
â”‚   â”‚   â””â”€â”€ wiki_parser.py        # XML parser for Wikipedia
â”‚   â”œâ”€â”€ processing/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ cleaner.py            # Text cleaning & normalization
â”‚   â”‚   â”œâ”€â”€ deduplicator.py       # MinHash LSH deduplication
â”‚   â”‚   â”œâ”€â”€ quality_filter.py     # Quality filtering
â”‚   â”‚   â””â”€â”€ tokenizer.py          # Tokenization & vocab
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ config.py             # Configuration management
â”‚   â”‚   â”œâ”€â”€ logger.py             # Logging utilities
â”‚   â”‚   â””â”€â”€ metrics.py            # Pipeline metrics tracking
â”‚   â””â”€â”€ main.py                   # Main pipeline orchestrator
â”œâ”€â”€ configs/
â”‚   â””â”€â”€ pipeline_config.yaml      # Pipeline configuration
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                      # Raw downloaded data
â”‚   â”œâ”€â”€ processed/                # Intermediate processed data
â”‚   â””â”€â”€ output/                   # Final output files
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ test_pipeline.py          # Unit tests
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ data_exploration.ipynb    # Data analysis notebook
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

## âš™ï¸ Configuration

Edit `configs/pipeline_config.yaml`:

```yaml
ingestion:
  source: "simplewiki"
  max_articles: 10000

cleaning:
  remove_citations: true
  min_length: 100

deduplication:
  num_perm: 128
  threshold: 0.8

quality:
  min_words: 50
  max_words: 100000
  language: "en"

tokenization:
  vocab_size: 32000
  model_type: "bpe"
```

## ğŸ“ˆ Metrics Dashboard

The pipeline tracks metrics at each stage:

```
============================================================
                    PIPELINE METRICS REPORT
============================================================

INGESTION
  Documents ingested:     50,000
  Total bytes:           245.6 MB
  Time elapsed:          2m 34s

CLEANING
  Documents processed:    50,000
  Avg chars removed:      12.3%
  Encoding fixes:         234

DEDUPLICATION
  Input documents:        50,000
  Duplicates found:       3,456
  Output documents:       46,544
  Dedup rate:            6.9%

QUALITY FILTER
  Input documents:        46,544
  Filtered (too short):   1,234
  Filtered (wrong lang):  567
  Output documents:       44,743

TOKENIZATION
  Documents tokenized:    44,743
  Total tokens:          89.4M
  Vocab size:            32,000
  Avg tokens/doc:        1,998

============================================================
```

## ğŸ§ª Testing

```bash
# Run all tests
pytest tests/

# Run with coverage
pytest tests/ --cov=src --cov-report=html
```

## ğŸ”§ Technologies Used

- **Python 3.9+** - Core language
- **mwparserfromhell** - Wikipedia markup parsing
- **datasketch** - MinHash LSH deduplication
- **langdetect** - Language detection
- **tokenizers** - HuggingFace tokenizers
- **pandas** - Data manipulation
- **tqdm** - Progress bars
- **PyYAML** - Configuration
- **Docker** - Containerization

## ğŸ“š References

- [The Pile: An 800GB Dataset of Diverse Text](https://arxiv.org/abs/2101.00027)
- [Deduplicating Training Data Makes Language Models Better](https://arxiv.org/abs/2107.06499)
- [Quality at a Glance: An Audit of Web-Crawled Multilingual Datasets](https://arxiv.org/abs/2103.12028)

## ğŸ“„ License

MIT License - see LICENSE file for details.

## ğŸ‘¤ Author

**Raguraja Krishnan Natarajan Mangaleshwaran**
- MS Information Systems (Applied Data Science) - SUNY Binghamton
- [LinkedIn](https://linkedin.com/in/yourprofile)
- [GitHub](https://github.com/yourprofile)
